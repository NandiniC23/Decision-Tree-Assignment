{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 1: What is a Decision Tree, and how does it work in the context of classification?**\n",
        "-  Decision TreeA Decision Tree is a type of supervised learning algorithm used for classification and regression tasks. It's a tree-like model that splits data into subsets based on features or attributes.\n",
        "\n",
        "How it WorksIn the context of classification, a Decision Tree works as follows:\n",
        "\n",
        "1. Root Node: The algorithm starts with a root node representing the entire dataset.\n",
        "2. Splitting: The algorithm selects a feature to split the data into subsets based on a specific criterion (e.g., Gini impurity or entropy).\n",
        "3. Child Nodes: Each subset of data is represented by a child node, and the process is repeated recursively until a stopping criterion is met (e.g., all instances in a node belong to the same class).\n",
        "4. Leaf Nodes: The final nodes in the tree are called leaf nodes, which represent the predicted class labels.\n",
        "\n",
        "#**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "-  Impurity Measures in Decision TreesIn Decision Trees, impurity measures are used to determine the best split for a node. Two common impurity measures are Gini Impurity and Entropy.\n",
        "\n",
        "Gini ImpurityGini Impurity measures the probability of misclassifying a randomly chosen instance from a node if it were randomly labeled according to the class distribution of the node. It's calculated as:\n",
        "\n",
        "Gini Impurity = 1 - Σ (p_i^2)\n",
        "\n",
        "Where:\n",
        "\n",
        "- p_i: Proportion of instances in the node that belong to class i\n",
        "\n",
        "EntropyEntropy measures the uncertainty or randomness in the class distribution of a node. It's calculated as:\n",
        "\n",
        "Entropy = - Σ (p_i * log2(p_i))\n",
        "\n",
        "Where:\n",
        "\n",
        "- p_i: Proportion of instances in the node that belong to class i\n",
        "\n",
        "Impact on SplitsBoth Gini Impurity and Entropy are used to evaluate the quality of a split in a Decision Tree. The goal is to find the split that results in the largest reduction in impurity.\n",
        "\n",
        "- Gini Impurity: A lower Gini Impurity value indicates a purer node. When splitting a node, the algorithm chooses the feature and split point that results in the largest reduction in Gini Impurity.\n",
        "- Entropy: A lower Entropy value indicates a more certain or less random class distribution. When splitting a node, the algorithm chooses the feature and split point that results in the largest reduction in Entropy.\n",
        "\n",
        "#**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "-  Pre-Pruning (Early Stopping)\n",
        "\n",
        "Definition: The tree stops growing early, before it becomes too complex. Rules are set beforehand to limit growth (e.g., max depth, minimum samples per split, minimum information gain).\n",
        "\n",
        "Idea: Prevent the tree from overfitting by stopping before it gets too specific.\n",
        "\n",
        "Practical Advantage:\n",
        "Faster training and simpler trees, since unnecessary branches are never created. Useful when working with large datasets or limited computation power.\n",
        "\n",
        "Post-Pruning (Prune After Full Growth)\n",
        "\n",
        "Definition: The tree is allowed to grow fully (possibly overfitting), then branches that do not improve generalization are cut back.\n",
        "\n",
        "Idea: Start with a complex model and then simplify it.\n",
        "\n",
        "Practical Advantage:\n",
        "Better accuracy and generalization, since pruning decisions are based on actual performance (validation data) instead of fixed early rules. Helpful when accuracy is more important than speed.\n",
        "\n",
        "#**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "-  Information Gain in Decision TreesInformation Gain is a measure used in Decision Trees to evaluate the quality of a split. It calculates the reduction in impurity or uncertainty in the target variable after splitting the data based on a particular feature.\n",
        "\n",
        "CalculationInformation Gain is calculated as the difference between the impurity of the parent node and the weighted average of the impurities of the child nodes.\n",
        "\n",
        "Information Gain = Impurity(Parent) - Σ (|Child|/|Parent| * Impurity(Child))\n",
        "\n",
        "Where:\n",
        "\n",
        "- Impurity(Parent): Impurity of the parent node\n",
        "- |Child|/|Parent|: Proportion of instances in the child node\n",
        "- Impurity(Child): Impurity of the child node\n",
        "\n",
        "Importance for Choosing the Best SplitInformation Gain is important for choosing the best split in a Decision Tree because it:\n",
        "\n",
        "- Evaluates the quality of the split: Information Gain helps evaluate the effectiveness of a split in reducing impurity or uncertainty in the target variable.\n",
        "- Compares different splits: Information Gain allows comparison of different splits and selection of the best one based on the largest reduction in impurity.\n",
        "- Improves model performance: By choosing the split with the highest Information Gain, the Decision Tree model can achieve better performance and accuracy.\n",
        "\n",
        "#**Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "-  Real-World Applications of Decision TreesDecision Trees have numerous real-world applications across various industries, including:\n",
        "\n",
        "1. Credit Risk Assessment: Decision Trees are used to evaluate creditworthiness and predict the likelihood of loan defaults.\n",
        "2. Medical Diagnosis: Decision Trees are used to diagnose diseases based on symptoms, medical history, and test results.\n",
        "3. Customer Segmentation: Decision Trees are used to segment customers based on demographic and behavioral attributes.\n",
        "4. Marketing and Advertising: Decision Trees are used to predict customer responses to marketing campaigns and personalize advertising.\n",
        "5. Fraud Detection: Decision Trees are used to detect fraudulent transactions and identify high-risk customers.\n",
        "\n",
        "Advantages of Decision TreesDecision Trees have several advantages:\n",
        "\n",
        "- Interpretability: Decision Trees are easy to understand and interpret, making them a popular choice for many applications.\n",
        "- Handling categorical features: Decision Trees can handle categorical features directly, eliminating the need for feature engineering.\n",
        "- Fast training: Decision Trees are relatively fast to train compared to other algorithms.\n",
        "- Handling missing values: Decision Trees can handle missing values in the data.\n",
        "\n",
        "Limitations of Decision TreesDespite their advantages, Decision Trees also have some limitations:\n",
        "\n",
        "- Overfitting: Decision Trees can suffer from overfitting, especially when the trees are deep or complex.\n",
        "- Not suitable for complex relationships: Decision Trees might not be the best choice for complex relationships between features.\n",
        "- Sensitive to noise: Decision Trees can be sensitive to noisy or irrelevant features.\n",
        "- Not suitable for high-dimensional data: Decision Trees can become complex and difficult to interpret when dealing with high-dimensional data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t82Ty3ouJkeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 6: Write a Python program to:**\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "ibjHM42Jh3d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\")\n",
        "\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1ClWRHAiHoj",
        "outputId": "daeb4a64-8a93-4855-e19f-36e6f68b8c31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 7: Write a Python program to:**\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "ni4gBBJljGWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "zfR7LnG3i2PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Decision Tree with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Fully-grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)  # no depth restriction\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree with max_depth=3 Accuracy:\", acc_limited)\n",
        "print(\"Fully-grown Decision Tree Accuracy:\", acc_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmjgMY1pjVDg",
        "outputId": "41e1dcf8-4736-47a2-e9ca-e330ef52bda7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with max_depth=3 Accuracy: 1.0\n",
            "Fully-grown Decision Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 8: Write a Python program to:**\n",
        "#● Load the California Housing dataset from sklearn\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "6dkXNoCQjj39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"\\nFeature Importances:\")\n",
        "\n",
        "for feature_name, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znF5PLgajWSQ",
        "outputId": "29301245-5167-41f1-a3c6-40ce4b34bce6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 9: Write a Python program to:**\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "oLpPxuTZj1d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree and GridSearchCV\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Model Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXZ5kHNIkGfu",
        "outputId": "42aa2e5e-bc39-4481-a802-0c5e9a9b74ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n",
        "# Explain the step-by-step process you would follow to:\n",
        "#● Handle the missing values\n",
        "#● Encode the categorical features\n",
        "#● Train a Decision Tree model\n",
        "#● Tune its hyperparameters\n",
        "#● Evaluate its performance\n",
        "#And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "-  \n",
        "Step-by-Step ProcessHere's a step-by-step process to handle missing values, encode categorical features, train a Decision Tree model, tune its hyperparameters, and evaluate its performance:\n",
        "\n",
        "Step 1: Handle Missing Values1. Identify missing values: Use pandas' isnull() function to identify missing values in the dataset.\n",
        "2. Determine the type of missing values: Determine whether the missing values are Missing At Random (MAR), Missing Completely At Random (MCAR), or Not Missing At Random (NMAR).\n",
        "3. Choose an imputation strategy: Based on the type of missing values and the dataset, choose an imputation strategy such as mean imputation, median imputation, or imputation using a regression model.\n",
        "4. Impute missing values: Use pandas' fillna() function or scikit-learn's Imputer class to impute missing values.\n",
        "\n",
        "Step 2: Encode Categorical Features1. Identify categorical features: Identify categorical features in the dataset.\n",
        "2. Choose an encoding strategy: Choose an encoding strategy such as one-hot encoding, label encoding, or ordinal encoding.\n",
        "3. Encode categorical features: Use pandas' get_dummies() function or scikit-learn's OneHotEncoder or LabelEncoder class to encode categorical features.\n",
        "\n",
        "Step 3: Train a Decision Tree Model1. Split the dataset: Split the dataset into a training set and a test set using scikit-learn's train_test_split() function.\n",
        "2. Train a Decision Tree model: Train a Decision Tree model using scikit-learn's DecisionTreeClassifier class.\n",
        "3. Make predictions: Make predictions on the test set using the trained model.\n",
        "\n",
        "Step 4: Tune Hyperparameters1. Define hyperparameters: Define hyperparameters to tune such as max_depth, min_samples_split, and min_samples_leaf.\n",
        "2. Use GridSearchCV or RandomizedSearchCV: Use scikit-learn's GridSearchCV or RandomizedSearchCV class to perform hyperparameter tuning.\n",
        "3. Evaluate performance: Evaluate the performance of the model with tuned hyperparameters.\n",
        "\n",
        "Step 5: Evaluate Performance1. Choose evaluation metrics: Choose evaluation metrics such as accuracy, precision, recall, F1-score, and AUC-ROC.\n",
        "2. Evaluate performance: Evaluate the performance of the model using the chosen metrics.\n",
        "3. Compare performance: Compare the performance of the model with and without hyperparameter tuning.\n",
        "\n",
        "Business ValueA Decision Tree model that predicts whether a patient has a certain disease can provide significant business value in the real-world setting:\n",
        "\n",
        "- Early detection: Early detection of diseases can lead to timely interventions, improved patient outcomes, and reduced healthcare costs.\n",
        "- Personalized medicine: The model can help personalize treatment plans based on individual patient characteristics.\n",
        "- Resource allocation: The model can help allocate resources more effectively by identifying high-risk patients and prioritizing their care.\n",
        "- Improved patient engagement: The model can help improve patient engagement by providing personalized recommendations and interventions.\n"
      ],
      "metadata": {
        "id": "wDXUbKiekNgG"
      }
    }
  ]
}